{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "name": " 2AdamW-DICE-BrainTumorGenesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyma-tas/Brain-Tumor-Segmentation-Project/blob/master/2AdamW_DICE_BrainTumorGenesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQHpMcFaMMJ"
      },
      "source": [
        "## Mount Colab to Drive\n",
        "\n",
        "This cell is to mount Colab to Drive, Colab is going to read data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8NGbyM98M2",
        "outputId": "6f343d60-a5e8-4564-e628-6e58c1c7b360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM3WQH_zaXsy"
      },
      "source": [
        "## Import Necessarry Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:09:57.562717Z",
          "start_time": "2020-07-22T19:09:55.836093Z"
        },
        "id": "Dd5zx2Cg9zn0",
        "outputId": "898bb645-d3e7-4078-e015-4116bcf60f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# read .mat files\n",
        "import h5py \n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Pytorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import SGD, lr_scheduler,AdamW\n",
        "\n",
        "#Train -test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# in Pytorch we define a device CPU or GPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L2FslF7adv9"
      },
      "source": [
        "## Accuracy and Loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVIrLAt96GR9"
      },
      "source": [
        "def dice_metric(inputs, target):\n",
        "    intersection = 2.0 * (target * inputs).sum()\n",
        "    union = target.sum() + inputs.sum()\n",
        "    if target.sum() == 0 and inputs.sum() == 0:\n",
        "        return 1.0\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "def dice_loss(inputs, target):\n",
        "    num = target.size(0)\n",
        "    inputs = inputs.reshape(num, -1)\n",
        "    target = target.reshape(num, -1)\n",
        "    smooth = 1.0\n",
        "    intersection = (inputs * target)\n",
        "    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
        "    dice = 1 - dice.sum() / num\n",
        "    return dice\n",
        "\n",
        "def bce_dice_loss(inputs, target):\n",
        "    dicescore = dice_loss(inputs, target)\n",
        "    bcescore = nn.BCELoss()\n",
        "    bceloss = bcescore(inputs, target)\n",
        "\n",
        "    return bceloss + dicescore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x3WcB7aam0H"
      },
      "source": [
        "## Models Genesis\n",
        "\n",
        "This is the structure from Models Genesis \n",
        "\n",
        "https://github.com/MrGiovanni/ModelsGenesis/blob/master/pytorch/unet3d.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXhLSjUmBsi8"
      },
      "source": [
        "class ContBatchNorm3d(nn.modules.batchnorm._BatchNorm):\n",
        "    def _check_input_dim(self, input):\n",
        "\n",
        "        if input.dim() != 5:\n",
        "            raise ValueError('expected 5D input (got {}D input)'.format(input.dim()))\n",
        "        #super(ContBatchNorm3d, self)._check_input_dim(input)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "        return F.batch_norm(\n",
        "            input, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "            True, self.momentum, self.eps)\n",
        "\n",
        "\n",
        "class LUConv(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, act):\n",
        "        super(LUConv, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_chan, out_chan, kernel_size=3, padding=1)\n",
        "        self.bn1 = ContBatchNorm3d(out_chan)\n",
        "\n",
        "        if act == 'relu':\n",
        "            self.activation = nn.ReLU(out_chan)\n",
        "        elif act == 'prelu':\n",
        "            self.activation = nn.PReLU(out_chan)\n",
        "        elif act == 'elu':\n",
        "            self.activation = nn.ELU(inplace=True)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "def _make_nConv(in_channel, depth, act, double_chnnel=False):\n",
        "    if double_chnnel:\n",
        "        layer1 = LUConv(in_channel, 32 * (2 ** (depth+1)),act)\n",
        "        layer2 = LUConv(32 * (2 ** (depth+1)), 32 * (2 ** (depth+1)),act)\n",
        "    else:\n",
        "        layer1 = LUConv(in_channel, 32*(2**depth),act)\n",
        "        layer2 = LUConv(32*(2**depth), 32*(2**depth)*2,act)\n",
        "\n",
        "    return nn.Sequential(layer1,layer2)\n",
        "\n",
        "class DownTransition(nn.Module):\n",
        "    def __init__(self, in_channel,depth, act):\n",
        "        super(DownTransition, self).__init__()\n",
        "        self.ops = _make_nConv(in_channel, depth,act)\n",
        "        self.maxpool = nn.MaxPool3d(2)\n",
        "        self.current_depth = depth\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.current_depth == 3:\n",
        "            out = self.ops(x)\n",
        "            out_before_pool = out\n",
        "        else:\n",
        "            out_before_pool = self.ops(x)\n",
        "            out = self.maxpool(out_before_pool)\n",
        "        return out, out_before_pool\n",
        "\n",
        "class UpTransition(nn.Module):\n",
        "    def __init__(self, inChans, outChans, depth,act):\n",
        "        super(UpTransition, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.up_conv = nn.ConvTranspose3d(inChans, outChans, kernel_size=2, stride=2)\n",
        "        self.ops = _make_nConv(inChans+ outChans//2,depth, act, double_chnnel=True)\n",
        "\n",
        "    def forward(self, x, skip_x):\n",
        "        out_up_conv = self.up_conv(x)\n",
        "        concat = torch.cat((out_up_conv,skip_x),1)\n",
        "        out = self.ops(concat)\n",
        "        return out\n",
        "\n",
        "\n",
        "class OutputTransition(nn.Module):\n",
        "    def __init__(self, inChans, n_labels):\n",
        "\n",
        "        super(OutputTransition, self).__init__()\n",
        "        self.final_conv = nn.Conv3d(inChans, n_labels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.sigmoid(self.final_conv(x))\n",
        "        return out\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    # the number of convolutions in each layer corresponds\n",
        "    # to what is in the actual prototxt, not the intent\n",
        "    def __init__(self, n_class=1, act='relu'):\n",
        "        super(UNet3D, self).__init__()\n",
        "\n",
        "        self.down_tr64 = DownTransition(1,0,act)\n",
        "        self.down_tr128 = DownTransition(64,1,act)\n",
        "        self.down_tr256 = DownTransition(128,2,act)\n",
        "        self.down_tr512 = DownTransition(256,3,act)\n",
        "\n",
        "        self.up_tr256 = UpTransition(512, 512,2,act)\n",
        "        self.up_tr128 = UpTransition(256,256, 1,act)\n",
        "        self.up_tr64 = UpTransition(128,128,0,act)\n",
        "        self.out_tr = OutputTransition(64, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out64, self.skip_out64 = self.down_tr64(x)\n",
        "        self.out128,self.skip_out128 = self.down_tr128(self.out64)\n",
        "        self.out256,self.skip_out256 = self.down_tr256(self.out128)\n",
        "        self.out512,self.skip_out512 = self.down_tr512(self.out256)\n",
        "\n",
        "        self.out_up_256 = self.up_tr256(self.out512,self.skip_out256)\n",
        "        self.out_up_128 = self.up_tr128(self.out_up_256, self.skip_out128)\n",
        "        self.out_up_64 = self.up_tr64(self.out_up_128, self.skip_out64)\n",
        "        self.out = self.out_tr(self.out_up_64)\n",
        "\n",
        "        return self.out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMC9WFE39zn7"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "To load \".mat\" images h5py library is imported. \n",
        "\n",
        "The shapes of MRI images are (512, 512) but there are 15 images whose shapes are (128, 128).  These 15 images are omitted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:28.407311Z",
          "start_time": "2020-07-22T19:11:28.358163Z"
        },
        "id": "4TGDfZuk9zoh",
        "outputId": "18dad751-71f5-40d3-9122-26d5fada709a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Load 3064 images, images are in .mat format, names are numbers.(1.mat, 2.mat ...)\n",
        "image_data = []\n",
        "mask_data = []\n",
        "\n",
        "filenames = range(1,3065)\n",
        "# Choose random number of images when needed\n",
        "# filenames = random.sample(filenames, 200) \n",
        "\n",
        "for name in filenames:\n",
        "    file = h5py.File('/content/drive/My Drive/Brain-Tumor-Segmentation-Project/brainTumorData/'+str(name)+'.mat', 'r').get('cjdata')\n",
        "   \n",
        "    input1 = file.get('image')[()]\n",
        "    mask = file.get('tumorMask')[()]\n",
        "\n",
        "    if input1.shape == (512, 512) and mask.shape == (512, 512):\n",
        "\n",
        "        input2 = np.reshape(input1,(64,64,64))\n",
        "        input3 = np.expand_dims(input2,axis=0)\n",
        "        image_data.append(input3)\n",
        "        mask = np.reshape(mask,(64,64,64))\n",
        "        mask = np.expand_dims(mask,axis=0)\n",
        "        mask_data.append(mask)\n",
        "\n",
        "    else: \n",
        "        print(name)\n",
        "# There are 15 images whose sizes are not suitable to the model. I omitted these images."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "955\n",
            "956\n",
            "957\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84LSLfgEdray"
      },
      "source": [
        "## Split the data into train, test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:33.862647Z",
          "start_time": "2020-07-22T19:11:33.852383Z"
        },
        "id": "KZxqP9aU9zoy",
        "outputId": "b993cd0e-58e3-42cf-d856-e27b814a77c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_train_data, image_test_data = train_test_split(image_data, test_size = 0.1, random_state=123)\n",
        "image_train_data, image_val_data = train_test_split(image_train_data, test_size = 0.111, random_state=123)\n",
        "print(len(image_train_data),len(image_test_data), len(image_val_data) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2439 305 305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:35.053429Z",
          "start_time": "2020-07-22T19:11:35.045016Z"
        },
        "id": "hTRnsx6B9zo1",
        "outputId": "dbaadbf9-aac9-42ce-e0a9-da5d1f57c95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mask_train_data, mask_test_data = train_test_split(mask_data, test_size = 0.1, random_state=123)\n",
        "mask_train_data, mask_val_data = train_test_split(mask_train_data, test_size = 0.111, random_state=123)\n",
        "print(len(mask_train_data),len(mask_test_data), len(mask_val_data) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2439 305 305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs3VKQCrd-uR"
      },
      "source": [
        "# Train, Validation and Test Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:36.521183Z",
          "start_time": "2020-07-22T19:11:36.332678Z"
        },
        "id": "1SVv1y3F9zo5"
      },
      "source": [
        "#Train Data\n",
        "image_train_data = torch.Tensor(image_train_data) # transform to torch tensor\n",
        "mask_train_data = torch.Tensor(mask_train_data)\n",
        "\n",
        "train_dataset = TensorDataset(image_train_data,mask_train_data) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset,batch_size= 4, num_workers=2, shuffle=False) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:39.059660Z",
          "start_time": "2020-07-22T19:11:38.988026Z"
        },
        "id": "bVa3pHO59zo8"
      },
      "source": [
        "# Test Data\n",
        "image_test_data = torch.Tensor(image_test_data) # transform to torch tensor\n",
        "mask_test_data = torch.Tensor(mask_test_data)\n",
        "\n",
        "test_dataset = TensorDataset(image_test_data,mask_test_data) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset,batch_size= 4, num_workers=2, shuffle=False) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-22T19:11:57.767096Z",
          "start_time": "2020-07-22T19:11:57.693941Z"
        },
        "id": "hbDbPOAQ9zo_"
      },
      "source": [
        "#Validation Data\n",
        "image_val_data = torch.Tensor(image_val_data) # transform to torch tensor\n",
        "mask_val_data = torch.Tensor(mask_val_data)\n",
        "\n",
        "val_dataset = TensorDataset(image_val_data,mask_val_data) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset,batch_size= 4, num_workers=2, shuffle=False) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooyiuKxveQKJ"
      },
      "source": [
        "# Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX3emcy5aOwL",
        "outputId": "429af6c4-1ed8-4a5d-9c9c-5814be68e509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# https://github.com/MrGiovanni/ModelsGenesis/tree/master/pytorch\n",
        "model = UNet3D()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet3D(\n",
              "  (down_tr64): DownTransition(\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (down_tr128): DownTransition(\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (down_tr256): DownTransition(\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (down_tr512): DownTransition(\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (up_tr256): UpTransition(\n",
              "    (up_conv): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_tr128): UpTransition(\n",
              "    (up_conv): ConvTranspose3d(256, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_tr64): UpTransition(\n",
              "    (up_conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "    (ops): Sequential(\n",
              "      (0): LUConv(\n",
              "        (conv1): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): LUConv(\n",
              "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "        (bn1): ContBatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out_tr): OutputTransition(\n",
              "    (final_conv): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAAIHdsIeaEj"
      },
      "source": [
        "## The pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGuSHXply4OD",
        "outputId": "72832180-8398-4ec1-d224-abf0b4ee68ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Define the file keeping the pre-trained weights \n",
        "# Genesis_Chest_CT.pt is a file I downloaded with the permission of Model Genesis\n",
        "weight_dir = '/content/drive/My Drive/Brain-Tumor-Segmentation-Project/Genesis_Chest_CT.pt'\n",
        "\n",
        "# Load the weights of Model Genesis\n",
        "checkpoint = torch.load(weight_dir, map_location=torch.device('cpu'))\n",
        "state_dict = checkpoint['state_dict']\n",
        "# Initialize a dictionary to store weights\n",
        "unParalled_state_dict = {}\n",
        "# Store weights in unParalled_state_dict\n",
        "for key in state_dict.keys():\n",
        "    unParalled_state_dict[key.replace(\"module.\", \"\")] = state_dict[key]\n",
        "# Load the new dictionary to the model\n",
        "model.load_state_dict(unParalled_state_dict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_7rsw1VetKo"
      },
      "source": [
        "## Define parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXf-_OFexUI"
      },
      "source": [
        "#Define the criterion\n",
        "criterion = bce_dice_loss\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.1)\n",
        "# Change the learning rate to reach the global minimum\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvIdZIgbfDX7"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OxxIEoa4P3",
        "outputId": "3981b714-07fb-453e-e43c-b9d95c8a493a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Move the model to GPU \n",
        "model.to(device)\n",
        "\n",
        "# Initialize lists to store loss values\n",
        "loss_history = []\n",
        "loss_history_val = []\n",
        "\n",
        "best_loss_val = float('inf')\n",
        "\n",
        "# Train\n",
        "print(\"Start train...\")\n",
        "for epoch in range(50):\n",
        "   #Train mode\n",
        "    model.train()\n",
        "    loss_running = []\n",
        "    for _, (x,y) in enumerate(train_dataloader):\n",
        "        x, y = x.float().to(device), y.float().to(device)\n",
        "        \n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "        loss_running.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "       \n",
        "    loss_history.append(np.mean(loss_running))\n",
        "    # Evaluate mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_val_running = []\n",
        "        for _, (x_val, y_val) in enumerate(val_dataloader):\n",
        "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "            pred_val = model.forward(x_val) #pred_val = model(x_val)\n",
        "            loss_val= criterion(pred_val, y_val)\n",
        "            loss_val_running.append(loss_val.item())\n",
        "    \n",
        "    curr_loss_val = np.mean(loss_val_running)\n",
        "    loss_history_val.append(curr_loss_val)\n",
        "    # Save the best weights\n",
        "    if curr_loss_val < best_loss_val:\n",
        "        best_loss_val = curr_loss_val\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Brain-Tumor-Segmentation-Project/best_model.pth')\n",
        "    # Change the learning rate\n",
        "    scheduler.step()\n",
        "    # Print the results\n",
        "    print(\"epoch\", epoch, \"train loss\", loss_history[-1], \"val loss\", loss_history_val[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start train...\n",
            "epoch 0 train loss 0.9750198336898304 val loss 0.9472322549138751\n",
            "epoch 1 train loss 0.8770960611886666 val loss 0.8264056815729512\n",
            "epoch 2 train loss 0.8104350524359062 val loss 0.7786627853071535\n",
            "epoch 3 train loss 0.7722033008688786 val loss 0.7684954346774461\n",
            "epoch 4 train loss 0.7563204318773551 val loss 0.7626414922150698\n",
            "epoch 5 train loss 0.7057667979451476 val loss 0.6936392524799744\n",
            "epoch 6 train loss 0.6887377186632547 val loss 0.6984348324212161\n",
            "epoch 7 train loss 0.6803527646377439 val loss 0.6699143632665857\n",
            "epoch 8 train loss 0.6741126392219887 val loss 0.699267801526305\n",
            "epoch 9 train loss 0.6690489853014712 val loss 0.6775594036300461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBLdgakWhavX"
      },
      "source": [
        "## Load the saved best model's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBlWJC66WEMq"
      },
      "source": [
        "checkpoint = torch.load('/content/drive/My Drive/Brain-Tumor-Segmentation-Project/best_model.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41jWY3TMhp7c"
      },
      "source": [
        "## Function to plot the mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5W8PPaTO2RA"
      },
      "source": [
        "def plot_mask(mask_3d_array, axx): # takes 64*64*64 array\n",
        "    mask_cpu = mask_3d_array.cpu().detach().numpy()\n",
        "    reshaped_mask_cpu = np.reshape(mask_cpu,(512, 512))\n",
        "    print(np.max(reshaped_mask_cpu), np.min(reshaped_mask_cpu))\n",
        "    reshaped_mask_cpu_bin = np.round(reshaped_mask_cpu)\n",
        "    axx.imshow(reshaped_mask_cpu_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3-GX9phymx"
      },
      "source": [
        "# Plot 4 random predicted and ground truth masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjEwS-IuwaQc"
      },
      "source": [
        "dataloader = test_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model.forward(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxPRAQG86jq0"
      },
      "source": [
        "dataloader = test_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SHxwJBw7IJd"
      },
      "source": [
        "dataloader = test_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfWkV71p7Jyy"
      },
      "source": [
        "dataloader = test_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdwvez-I6kIS"
      },
      "source": [
        "dataloader = train_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n8RiSr77Ljr"
      },
      "source": [
        "dataloader = test_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0cbcrLk6_Er"
      },
      "source": [
        "dataloader = train_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model.forward(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URgDl28_6jTs"
      },
      "source": [
        "dataloader = val_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY-SsJ4y65dR"
      },
      "source": [
        "dataloader = val_dataloader\n",
        "ncol = 4\n",
        "rand_ndx = random.sample(range(0, len(dataloader)), ncol)\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=ncol, figsize=(20, 10))\n",
        "i = 0\n",
        "for n, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if n in rand_ndx:\n",
        "        pred = model(x)\n",
        "        plot_mask(pred[0,0,:,:,:], ax[0][i])\n",
        "        plot_mask(y[0,0,:,:,:], ax[1][i])\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oopQYcPrlaZw"
      },
      "source": [
        "## Heatmap of one predicted mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytg9J5KFKPtj"
      },
      "source": [
        "mask_cpu = pred.cpu().detach().numpy()\n",
        "reshaped_mask_cpu = np.reshape(mask_cpu[0,0,:,:,:],(512, 512))\n",
        "\n",
        "reshaped_mask_cpu += .1\n",
        "reshaped_mask_cpu_bin = np.round(reshaped_mask_cpu)\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(reshaped_mask_cpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUKb38r-kcaN"
      },
      "source": [
        "## BCE-Dice Loss and Dice Accuracy Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek2RcXMDgMNA"
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(loss_history)\n",
        "plt.plot(loss_history_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyf79j_kvIp"
      },
      "source": [
        "## Function to compute the dice accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQu_-jd_jIya"
      },
      "source": [
        "def compute_acc(dataloader, model):\n",
        "    acc = []\n",
        "    loss = []\n",
        "    #model.eval()\n",
        "    #with torch.no_grad():\n",
        "    for _, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x)\n",
        "        loss.append(bce_dice_loss(pred, y).item())\n",
        "        # loss.append(BCELoss)    \n",
        "        acc.append(dice_metric(pred.data.cpu().numpy(), y.data.cpu().numpy()))\n",
        "       \n",
        "    print(np.mean(loss), np.mean(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaiCY99Zk34G"
      },
      "source": [
        "## Compute dice accuracy for train, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxX9eVG91wvi"
      },
      "source": [
        "compute_acc(test_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_hmCLvB0gea"
      },
      "source": [
        "compute_acc(train_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KKwZ3XD0i8B"
      },
      "source": [
        "compute_acc(val_dataloader, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}